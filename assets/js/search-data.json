{
  
    
        "post0": {
            "title": "Patent network analysis",
            "content": "import pandas as pd import string as str import regex as re from google.colab import drive drive.mount(&#39;/content/drive&#39;) . df = pd.read_csv(&#39;/content/drive/MyDrive/Colab Notebooks/DSBA Patent Project /Paper /test NPA/patbase_export_296387709.csv&#39;) . EDA . df.columns . Index([&#39;Family Number&#39;, &#39;Earliest Priority Date&#39;, &#39;Extended Family Number&#39;, &#39;Calculated Expiry Date&#39;, &#39;Earliest Priority Country&#39;, &#39;Original Patent Number&#39;, &#39;Original Kind Code&#39;, &#39;Original Application Number&#39;, &#39;DOCDB Patent Number&#39;, &#39;DOCDB Kind Code&#39;, &#39;DOCDB Application Number&#39;, &#39;Earliest Publication Number&#39;, &#39;Earliest Publication Date&#39;, &#39;Assignees&#39;, &#39;Inventors&#39;, &#39;Inventor Details Name&#39;, &#39;Inventor Details Std/Non Std&#39;, &#39;Inventor Details Address&#39;, &#39;Assignee Details Name&#39;, &#39;Assignee Details Std/Non Std&#39;, &#39;Assignee Details Address&#39;, &#39;Agent&#39;, &#39;Agent Details Name&#39;, &#39;Agent Details Address&#39;, &#39;Title&#39;, &#39;Abstract&#39;, &#39;1st Main Claim&#39;, &#39;Number of Claims&#39;, &#39;Number of Independent Claims&#39;, &#39;Backward Citations&#39;, &#39;Forward Citations&#39;, &#39;Number of Backward Citations&#39;, &#39;Number of Forward Citations&#39;, &#39;NPL Citations&#39;, &#39;Number of NPL Citations&#39;, &#39;Cooperative Patent Class&#39;, &#39;International Class&#39;, &#39;US Class&#39;, &#39;Patent Type&#39;, &#39;Grant&#39;], dtype=&#39;object&#39;) . print(df.shape) . (3762, 40) . df.head(3) . Family Number Earliest Priority Date Extended Family Number Calculated Expiry Date Earliest Priority Country Original Patent Number Original Kind Code Original Application Number DOCDB Patent Number DOCDB Kind Code DOCDB Application Number Earliest Publication Number Earliest Publication Date Assignees Inventors Inventor Details Name Inventor Details Std/Non Std Inventor Details Address Assignee Details Name Assignee Details Std/Non Std Assignee Details Address Agent Agent Details Name Agent Details Address Title Abstract 1st Main Claim Number of Claims Number of Independent Claims Backward Citations Forward Citations Number of Backward Citations Number of Forward Citations NPL Citations Number of NPL Citations Cooperative Patent Class International Class US Class Patent Type Grant . 0 20652658 | 19961031 | NaN | 20171031.0 | DK | US06020349 | A | US08962098 | US6020349 | A | US96209897 | US6020349 A | 20000201 | NOVO NORDISK AS | ANKERSEN MICHAEL; STIDSEN CARSTEN ENGGAARD; CR... | ANKERSEN MICHAEL; STIDSEN CARSTEN ENGGAARD; CR... | Standard; Standard; Standard; Standard | DK; DK; US; DK | NOVO NORDISK AS; NOVO NORDISK A S | Standard; Non Standard | DK; | NaN | NaN | NaN | [EN] CONSTRAINED SOMATOSTATIN AGONISTS AND ANT... | [EN] The present invention relates to a compou... | [EN] 1. A compound of formula I: wherein A is ... | 20 | 1 | DE3631334 A1; EP0304330 A1; EP0448765 A1; US42... | US2003195187 AA; US2003207814 AA; US2003229025... | 6 | 40 | Woderer et al., Chem. Ber. 119, 2050-2054, (19... | 2 | A61P1/00; A61P11/00; A61P25/00; A61P27/02; A61... | A61K31/44; A61K31/4439; A61K31/445; A61K31/454... | 514/253.01; 514/318; 514/340; 514/341; 514/866... | NaN | YES | . 1 20652658 | 19961031 | NaN | 20171031.0 | DK | US06083960 | A | US09397355 | US6083960 | A | US39735599 | US6083960 A | 20000704 | NOVO NORDISK AS | ANKERSEN MICHAEL; STIDSEN CARSTEN ENGGAARD; CR... | ANKERSEN MICHAEL; STIDSEN CARSTEN ENGGAARD; CR... | Standard; Standard; Standard; Standard | DK; DK; US; DK | NOVO NORDISK AS; NOVO NORDISK A S | Standard; Non Standard | DK; | NaN | NaN | NaN | [EN] CONSTRAINED SOMATOSTATIN AGONISTS AND ANT... | [EN] The present invention relates to a compou... | [EN] 1. A compound formula I wherein A is pyri... | 11 | 1 | DE3631334 A1; EP0304330 A1; EP0448765 A1; US42... | US2004106625 AA; US2008153835 AA; US7279493 BB... | 6 | 4 | Woderer et al., Chem. Ber. 119, 2050-2054, (19... | 2 | A61P1/00; A61P11/00; A61P25/00; A61P27/02; A61... | A61K31/44; A61K31/4439; A61K31/445; A61K31/454... | 514/253.01; 514/318; 514/338; 514/344; 514/866... | NaN | YES | . 2 15295978 | 19990817 | NaN | 20190826.0 | DK | US06586574 | B1 | US09638590 | US6586574 | B1 | US63859000 | US6586574 BA | 20030701 | NN AS | HANSEN LARS LINDGAARD | HANSEN LARS LINDGAARD; LARS LINDGAARD HANSEN | Standard; Non Standard | DK; GADSTRUP DK | NN AS; NN A S; NOVO NORDISK A S; NOVO NORDISK ... | Standard; Non Standard; Non Standard; Non Stan... | DK; BAGSVAERD DK; ; ; | MARC A BEGAN ESQ; REZA GREEN ESQ; RICHARD W BO... | MARC A BEGAN ESQ; REZA GREEN ESQ; RICHARD W BO... | ; ; | [EN] STABILIZATION OF FREEZE-DRIED CAKE | [EN] The invention relates to the use of glycy... | [EN] 1. A lyophilized composition comprising a... | 13 | 2 | EP0359201 A2; US3773626 A; US4297344 A; US4687... | CN107073082 A; EP3182993 A4; EP3750556 A1; US1... | 9 | 44 | Davis et al., Archives of Biochemistry and Bio... | 5 | A61K47/183; A61K47/26; A61K9/19; C12N9/96 | A61K47/16; A61K47/18; A61K47/26; A61K9/19; C12... | 252/363.5; 530/384; 514/971; 514/834; 514/802;... | NaN | YES | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; print(df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 3762 entries, 0 to 3761 Data columns (total 40 columns): # Column Non-Null Count Dtype -- -- 0 Family Number 3762 non-null int64 1 Earliest Priority Date 3762 non-null int64 2 Extended Family Number 154 non-null float64 3 Calculated Expiry Date 2810 non-null float64 4 Earliest Priority Country 3762 non-null object 5 Original Patent Number 3737 non-null object 6 Original Kind Code 3737 non-null object 7 Original Application Number 3737 non-null object 8 DOCDB Patent Number 3748 non-null object 9 DOCDB Kind Code 3748 non-null object 10 DOCDB Application Number 3748 non-null object 11 Earliest Publication Number 3762 non-null object 12 Earliest Publication Date 3762 non-null int64 13 Assignees 3586 non-null object 14 Inventors 3747 non-null object 15 Inventor Details Name 3761 non-null object 16 Inventor Details Std/Non Std 3761 non-null object 17 Inventor Details Address 3760 non-null object 18 Assignee Details Name 3762 non-null object 19 Assignee Details Std/Non Std 3762 non-null object 20 Assignee Details Address 3747 non-null object 21 Agent 1887 non-null object 22 Agent Details Name 1887 non-null object 23 Agent Details Address 882 non-null object 24 Title 3749 non-null object 25 Abstract 3743 non-null object 26 1st Main Claim 3747 non-null object 27 Number of Claims 3762 non-null int64 28 Number of Independent Claims 3762 non-null int64 29 Backward Citations 3351 non-null object 30 Forward Citations 3006 non-null object 31 Number of Backward Citations 3762 non-null int64 32 Number of Forward Citations 3762 non-null int64 33 NPL Citations 2328 non-null object 34 Number of NPL Citations 3762 non-null int64 35 Cooperative Patent Class 3733 non-null object 36 International Class 3746 non-null object 37 US Class 3463 non-null object 38 Patent Type 17 non-null object 39 Grant 3762 non-null object dtypes: float64(2), int64(8), object(30) memory usage: 1.1+ MB None . There are three variables that relate to the inventor-level, being . The inventor | Inventor Details Name | Inventor Details Address | . df[&#39;DOCDB Patent Number&#39;].nunique() # we could use this variable as a unique ID . 3748 . By means of EDA, we found out that we could use the above variable as a unique ID variable. . print(df.Inventors.head(1)) . 0 ANKERSEN MICHAEL; STIDSEN CARSTEN ENGGAARD; CR... Name: Inventors, dtype: object . Create the data set for the network . df1 = df.Inventors.str.split(&#39;;&#39;, expand = True) # creating a wide data set from the invetors column, placing each and every inventor in a different column df1.head(2) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . 1 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df1 = df1.join(df[&#39;DOCDB Patent Number&#39;]) # join also the column of unique IDs # df1 = df1.join(df[&#39;Inventor Details Address&#39;]) for the future df1.head(2) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 DOCDB Patent Number . 0 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6020349 | . 1 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6083960 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df1.head(10) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 DOCDB Patent Number . 0 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6020349 | . 1 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6083960 | . 2 HANSEN LARS LINDGAARD | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6586574 | . 3 KOCH KAREN | KVORNING INGELISE | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US7018992 | . 4 HOFSTAETTER THIBAUD | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US5860946 | . 5 ROJKJAER RASMUS | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US2003118580 | . 6 ROJKJAER RASMUS | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US2008075709 | . 7 AHMADIAN HALEH | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US2008268521 | . 8 PERSSON EGON | OLSEN OLE HVILSTED | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6911323 | . 9 SORENSEN BRIT BINOW | PETERSEN LARS CHRISTIAN | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6858587 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df1[&#39;DOCDB Patent Number&#39;].isnull().sum() . 14 . df1 = df1.dropna(subset=[&#39;DOCDB Patent Number&#39;]) df1.shape . (3748, 19) . new_col_list = [&#39;inventor1&#39;, &#39;inventor2&#39;, &#39;inventor3&#39;, &#39;inventor4&#39;, &#39;inventor5&#39;, &#39;inventor6&#39;, &#39;inventor7&#39;, &#39;inventor8&#39;, &#39;inventor9&#39;, &#39;inventor10&#39;, &#39;inventor11&#39;, &#39;inventor12&#39;, &#39;inventor13&#39;, &#39;inventor14&#39;, &#39;inventor15&#39;, &#39;inventor16&#39;, &#39;inventor17&#39;, &#39;inventor18&#39;, &#39;patent_number&#39;] . old_columns = list(df1) print(old_columns) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, &#39;DOCDB Patent Number&#39;] . df1.rename(columns = {old_columns[idx]: name for (idx, name) in enumerate (new_col_list)}, inplace = True) . df2 = pd.wide_to_long(df1, stubnames=&#39;inventor&#39;,i = &#39;patent_number&#39;, j = &#39;irrelevant&#39;) . df2 = df2.dropna() . df2.head(3) # here we can appreciate a dataframe where the inventor is indexed by the patent number . inventor . patent_number irrelevant . US6020349 1 ANKERSEN MICHAEL | . US6083960 1 ANKERSEN MICHAEL | . US6586574 1 HANSEN LARS LINDGAARD | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df2.shape # indeed the shape of such a dataframe is given by the product of the number of unique inventors times the time that they invented . (13253, 1) . df2 = df2.reset_index(level=[&#39;patent_number&#39;]) #reset_index transforms an index of a df into a features . df2.head() . patent_number inventor . irrelevant . 1 US6020349 | ANKERSEN MICHAEL | . 1 US6083960 | ANKERSEN MICHAEL | . 1 US6586574 | HANSEN LARS LINDGAARD | . 1 US7018992 | KOCH KAREN | . 1 US5860946 | HOFSTAETTER THIBAUD | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df2[&#39;inventor_id&#39;] = df2[&#39;inventor&#39;].factorize()[0] #create a new column with a id per each and every unique inventor . df2.head(6) . patent_number inventor inventor_id . irrelevant . 1 US6020349 | ANKERSEN MICHAEL | 0 | . 1 US6083960 | ANKERSEN MICHAEL | 0 | . 1 US6586574 | HANSEN LARS LINDGAARD | 1 | . 1 US7018992 | KOCH KAREN | 2 | . 1 US5860946 | HOFSTAETTER THIBAUD | 3 | . 1 US2003118580 | ROJKJAER RASMUS | 4 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df2[&#39;patent_number&#39;].nunique() # this number is 1 less than some cells before . 3747 . Edge list . We can create an edge list merging the dataframe with itself using the patent_number, so that, each and every row represents the coinvention of two inventors of the same patent. The only thing that we then need to remove are self-links since a person can not really invent with herself. . edges = pd.merge(df2, df2, on = &#39;patent_number&#39;) . edges.head(2) # talking about self-links, you have an example just here . patent_number inventor_x inventor_id_x inventor_y inventor_id_y . 0 US6020349 | ANKERSEN MICHAEL | 0 | ANKERSEN MICHAEL | 0 | . 1 US6020349 | ANKERSEN MICHAEL | 0 | STIDSEN CARSTEN ENGGAARD | 1487 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; edges = edges[edges.inventor_id_x != edges.inventor_id_y] . . We are now in a situation where people that co-invented a patent will have at least one row per invention. This can be aggregated in the following way by grouping. . # How many times did Person X and Person Y co-invented together # reset_index makes everytging from a multi-index-series into a dataframe edges = edges.groupby([&#39;inventor_id_x&#39;, &#39;inventor_id_y&#39;]).size().reset_index() . edges.head(2) . inventor_id_x inventor_id_y 0 . 0 0 | 1487 | 3 | . 1 0 | 1520 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; The column called &quot;0&quot; counts how many time a person x &amp; a person y co-invent . edges.shape . (29532, 3) . import os import matplotlib.pyplot as plt import seaborn as sns . dist_coinv = edges[0].value_counts() # distribution of the number of co-inventions, i.e., there are 5270 pair of inventors that have co-invented 2 patents. dist_coinv = dist_coinv[:7, ] plt.figure(figsize = (10,5)) sns.barplot(dist_coinv.index, dist_coinv.values, alpha = 0.8) plt.title(&#39;Distribution of the occurrences of co-inventing&#39;) plt.ylabel(&#39;Number of occurrences&#39;, fontsize=12) plt.xlabel(&#39;Number of co-invention&#39;, fontsize=12) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . edges.rename({0:&#39;weight&#39;}, axis = 1, inplace=True) . Creating the Graph object with NetworkX . Now we can create a network object from this edgelist. . import networkx as nx from community import community_louvain . # The nodes are the inventors and the edges are weighted by the number of co-invention occured between two inventors G = nx.from_pandas_edgelist(edges, source=&#39;inventor_id_x&#39;, target=&#39;inventor_id_y&#39;, edge_attr=&#39;weight&#39;, create_using = nx.Graph()) . df2[[&#39;inventor_id&#39;,&#39;inventor&#39;]] . inventor_id inventor . irrelevant . 1 0 | ANKERSEN MICHAEL | . 1 0 | ANKERSEN MICHAEL | . 1 1 | HANSEN LARS LINDGAARD | . 1 2 | KOCH KAREN | . 1 3 | HOFSTAETTER THIBAUD | . ... ... | ... | . 15 4135 | KUKI ATSUO | . 16 4136 | VANOVA HANA | . 16 4137 | TESTON KIMBERLY ANN | . 17 4129 | TENG MIN | . 18 4134 | KIEL DAN | . 13253 rows √ó 2 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df2[[&#39;inventor_id&#39;,&#39;inventor&#39;]].set_index(&#39;inventor_id&#39;) . inventor . inventor_id . 0 ANKERSEN MICHAEL | . 0 ANKERSEN MICHAEL | . 1 HANSEN LARS LINDGAARD | . 2 KOCH KAREN | . 3 HOFSTAETTER THIBAUD | . ... ... | . 4135 KUKI ATSUO | . 4136 VANOVA HANA | . 4137 TESTON KIMBERLY ANN | . 4129 TENG MIN | . 4134 KIEL DAN | . 13253 rows √ó 1 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; node_attributes = df2[[&#39;inventor_id&#39;,&#39;inventor&#39;]].set_index(&#39;inventor_id&#39;).drop_duplicates().to_dict(&#39;index&#39;) . nx.set_node_attributes(G, {G.degree(): &#39;degree&#39;}) . nx.set_node_attributes(G, node_attributes) . len(G.nodes()) . 3961 . len(G.edges()) . 14766 . Centrality indicators . From here we will calculate various centrality measures and perform community detection. Think about the latter as UML (which it actually is). This will allow us to investigate e.g.: . Who are the top people in these communities? | . centrality_dgr = nx.degree_centrality(G) centrality_eig = nx.eigenvector_centrality_numpy(G) partition = community_louvain.best_partition(G) #that will take some time... degree = G.degree() . nx.set_node_attributes(G, centrality_dgr, &#39;dgr&#39;) nx.set_node_attributes(G, centrality_eig, &#39;eig&#39;) nx.set_node_attributes(G, partition, &#39;partition&#39;) nx.set_node_attributes(G, dict(degree), &#39;degree_basic&#39;) . Bringing it back to pandas . Once all graph indicators are in place, we can bring them back to Pandas for easier further analysis üßê. You can compare that step to inspecting individual clusters identified with e.g. K-means. . nodes_df = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient=&#39;index&#39;) . nodes_df.sort_values(&#39;eig&#39;, ascending=False)[:10] . inventor dgr eig partition degree_basic . 1655 LAU JESPER | 0.047727 | 0.322390 | 48 | 189 | . 1558 MADSEN PETER | 0.038636 | 0.223897 | 11 | 153 | . 1762 KODRA JANOS TIBOR | 0.025505 | 0.207342 | 48 | 101 | . 1763 BEHRENS CARSTEN | 0.024495 | 0.165824 | 48 | 97 | . 2359 GARIBAY PATRICK WILLIAM | 0.017424 | 0.138849 | 6 | 69 | . 2052 THOEGERSEN HENNING | 0.022475 | 0.135706 | 6 | 89 | . 2489 BLOCH PAW | 0.014646 | 0.131985 | 48 | 58 | . 1630 ANDERSEN HENRIK SUNE | 0.024495 | 0.127523 | 11 | 97 | . 1519 ANKERSEN MICHAEL | 0.015657 | 0.127359 | 48 | 62 | . 1823 MADSEN KJELD | 0.018687 | 0.122319 | 48 | 74 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; nodes_df.partition.nunique() . 284 . dist_part = nodes_df.partition.value_counts()[:10] plt.figure(figsize=(10,5)) sns.barplot(dist_part.index, dist_part.values, alpha=0.8) plt.title(&#39;Distribution of inventors amoing the top largest 10 partitions&#39;) plt.ylabel(&#39;Number of Occurrences&#39;, fontsize=12) plt.xlabel(&#39;Partitions&#39;, fontsize=12) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . top5_part = nodes_df.partition.value_counts()[:5].index . top5_part_nodes = nodes_df[nodes_df.partition.isin(top5_part)].index . G_sub = nx.subgraph(G, top5_part_nodes) . nodes_df_top5 = nodes_df[nodes_df.partition.isin(top5_part)] . nodes_df_top5.head(2) . inventor dgr eig partition degree_basic . 0 ANKERSEN MICHAEL | 0.002273 | 0.023971 | 48 | 9 | . 1487 STIDSEN CARSTEN ENGGAARD | 0.001010 | 0.000872 | 48 | 4 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; # with the highest eigenvector centrality top_people = nodes_df_top5.groupby(&#39;partition&#39;)[&#39;eig&#39;].nlargest(5).reset_index() . top_people.rename({&#39;level_1&#39;:&#39;inventor_id&#39;}, axis=1, inplace=True) . top_people = pd.merge(top_people, df2[[&#39;inventor&#39;,&#39;inventor_id&#39;]].drop_duplicates(), on=&#39;inventor_id&#39;, how=&#39;inner&#39;) . top_people.head(50) . partition inventor_id eig inventor . 0 6 | 2359 | 0.138849 | GARIBAY PATRICK WILLIAM | . 1 6 | 2052 | 0.135706 | THOEGERSEN HENNING | . 2 6 | 1595 | 0.116120 | HOEG JENSEN THOMAS | . 3 6 | 1820 | 0.102756 | LAU JESPER F | . 4 6 | 1933 | 0.095319 | KRUSE THOMAS | . 5 12 | 1511 | 0.094768 | SAUERBERG PER | . 6 12 | 1531 | 0.089727 | PETTERSSON INGRID | . 7 12 | 1571 | 0.068005 | PETERSEN ANDERS KLARSKOV | . 8 12 | 1555 | 0.062463 | OLESEN PREBEN HOULBERG | . 9 12 | 449 | 0.052110 | KRUSE THOMAS | . 10 13 | 2089 | 0.002126 | BENGTSSON HENRIK | . 11 13 | 2030 | 0.002008 | EILERTSEN LARS | . 12 13 | 2087 | 0.001870 | DRUSTRUP JOERN | . 13 13 | 1028 | 0.000233 | JEPPESEN PER | . 14 13 | 2355 | 0.000225 | ANDRESEN LENE | . 15 21 | 1709 | 0.028325 | BREINHOLT JENS | . 16 21 | 515 | 0.025431 | JONASSEN IB | . 17 21 | 22 | 0.006666 | CHRISTENSEN THORKILD | . 18 21 | 2570 | 0.005346 | PATKAR SHAMKANT ANANT | . 19 21 | 1690 | 0.004938 | SVENDSEN ALLAN | . 20 48 | 1655 | 0.322390 | LAU JESPER | . 21 48 | 1762 | 0.207342 | KODRA JANOS TIBOR | . 22 48 | 1763 | 0.165824 | BEHRENS CARSTEN | . 23 48 | 2489 | 0.131985 | BLOCH PAW | . 24 48 | 1519 | 0.127359 | ANKERSEN MICHAEL | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Now we can explore the names, happy stalking. .",
            "url": "https://themuahdibportfolio.github.io/Blog/network%20analysis/intellectual%20property/patent%20data/2022/02/08/_02_08_PNA-(2).html",
            "relUrl": "/network%20analysis/intellectual%20property/patent%20data/2022/02/08/_02_08_PNA-(2).html",
            "date": " ‚Ä¢ Feb 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Predict the Cooperative Patent Class (CPC) by means of NLP",
            "content": "Imports . import pandas as pd # data analysis from io import StringIO import matplotlib.pyplot as plt import numpy as np import seaborn as sns %matplotlib inline sns.set(color_codes=True) . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.feature_selection import chi2 . Load the data . The data source of the patent-level data is PatBase, that is a product offered by MineSoft, i.e., patent solutions provider founded in 1996 offering online products and services, such as, patent research, monitoring, and analysis, as well as other intellectual property services. . df = pd.read_csv(&#39;/content/drive/MyDrive/Colab Notebooks/DSBA Patent Project /EDA &amp; NLP /patbase_export_274777127.csv&#39;) ## df.head() . Subset the data . The only two features needed in this prediction exercise are the Cooperative Patent Class and the text of the 1&#39; Main Claim. . col = [&#39;Cooperative Patent Class&#39;, &#39;1st Main Claim&#39;] #we only use those two columns df = df[col] . df = df.dropna() . Text manipulation . We can see that each patent publication, i.e., each row, has multiple CPC labels. For convinience, we only use the first label in order of appearance. . df[&#39;cpc&#39;] = df[&#39;Cooperative Patent Class&#39;].str.extract(r&#39;(^.{0,1})&#39;) . Preprocessing of text of the claim . df[&#39;main_claim&#39;] = df[&#39;1st Main Claim&#39;].str.replace(&#39; [EN ] s1. s&#39;, &#39;&#39;, regex = True) # string stripping EN . EDA of the CPC . We found that there are 8 unique classes among the all data set. . df[&#39;cpc&#39;].nunique() . 8 . Subsetting the data to allow Colab to process the command . df = df.sample(frac = 0.2) . df[&#39;category_id&#39;] = df[&#39;cpc&#39;].factorize()[0] # we add a column encodnig the cpc as # an integer because categorical variables are better represented by integers than strings. # cosa corrisponde a cosa???? . category_id_df = df[[&#39;cpc&#39;, &#39;category_id&#39;]].drop_duplicates().sort_values(&#39;category_id&#39;) . category_id_df . cpc category_id . 3368 G | 0 | . 713 H | 1 | . 4842 A | 2 | . 18950 C | 3 | . 18037 B | 4 | . 23669 F | 5 | . 21886 E | 6 | . 10205 D | 7 | . category_to_id = dict(category_id_df.values) # a dictionary of category to id integer . category_to_id . {&#39;A&#39;: 2, &#39;B&#39;: 4, &#39;C&#39;: 3, &#39;D&#39;: 7, &#39;E&#39;: 6, &#39;F&#39;: 5, &#39;G&#39;: 0, &#39;H&#39;: 1} . id_to_category = dict(category_id_df[[&#39;category_id&#39;, &#39;cpc&#39;]].values) # a dictionary of id integer to category . id_to_category . {0: &#39;G&#39;, 1: &#39;H&#39;, 2: &#39;A&#39;, 3: &#39;C&#39;, 4: &#39;B&#39;, 5: &#39;F&#39;, 6: &#39;E&#39;, 7: &#39;D&#39;} . fig = plt.figure(figsize=(16,5)) df.groupby(&#39;cpc&#39;).main_claim.count().plot.bar(ylim = 0) plt.show() . fig = plt.figure(figsize=(16,5)) df.groupby(&#39;category_id&#39;).main_claim.count().plot.bar(ylim = 0) plt.show() . tfidf = TfidfVectorizer(sublinear_tf=True, # Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf). min_df=5,# minimum numbers of documents a word must be present in to be kept. norm=&#39;l2&#39;, # I am not sure about this parameter encoding=&#39;latin-1&#39;, # I am not sure about this parameter ngram_range=(1, 2), # consider both unigrams and bigrams. stop_words=&#39;english&#39;) # remove all common pronouns features = tfidf.fit_transform(df.main_claim).toarray labels = df[&#39;cpc&#39;] features.shape . (6203, 14642) . features_names = tfidf.get_feature_names() . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) . features1 = tfidf.fit_transform(df.main_claim) . dense = features1.todense() . denselist = dense.tolist() . df_features = pd.DataFrame(denselist, columns= features_names) . df_features.head(3) . 00 000 001 01 01 percent 02 05 10 10 degrees 10 min 10 nm 10 percent 100 100 parts 100 percent 1000 11 110 111 12 120 125 13 14 140 15 15 percent 150 16 17 18 180 180 degrees 19 20 20 degrees 20 percent 20 wt 200 21 ... working fluid workload workpiece world worn wound woven wrap wrapped wrapping wraps wrist write write command write data write operation write request writes writing writing data written written second wt wt percent x1 x2 x3 yield yl yoke zero zinc zirconium zn zn combinations zone zone based zones zoom zooming . 0 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 rows √ó 14642 columns . Prediction of CPC label using Naive Bayes . from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.feature_extraction.text import TfidfTransformer from sklearn.naive_bayes import MultinomialNB X_train, X_test, y_train, y_test = train_test_split(df[&#39;main_claim&#39;], df[&#39;cpc&#39;], random_state = 0) count_vect = CountVectorizer() X_train_counts = count_vect.fit_transform(X_train) tfidf_transformer = TfidfTransformer() X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) MultinomialNB = MultinomialNB().fit(X_train_tfidf, y_train) . print(MultinomialNB.predict(count_vect.transform([&quot;A user device comprising: one or more processors; anda computer-readable medium storing a plurality of computer-executable components that are executable by the one or more processors to perform a plurality of actions, the plurality of actions comprising:receiving supplemental scheduling information, the supplemental scheduling information being generated based on telemetry data collected from a plurality of access points of a data network;requesting an access point of the data network for accessing a wireless local area network according to a wireless usage type, the wireless usage type based on an application running on the user device and the supplemental scheduling information; andreceiving an allocation of time-frequency divisions for accessing the wireless local area network from the access point.&quot;]))) . [&#39;G&#39;] . Selection of various model . from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.svm import LinearSVC from sklearn.model_selection import cross_val_score models = [ RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), LinearSVC(), MultinomialNB(), LogisticRegression(random_state=0), ] . CV = 5 cv_df = pd.DataFrame(index=range(CV * len(models))) entries = [] for model in models: model_name = model.__class__.__name__ accuracies = cross_val_score(model, features, labels, scoring=&#39;accuracy&#39;, cv=CV) for fold_idx, accuracy in enumerate(accuracies): entries.append((model_name, fold_idx, accuracy)) cv_df = pd.DataFrame(entries, columns=[&#39;model_name&#39;, &#39;fold_idx&#39;, &#39;accuracy&#39;]) . /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, /usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG, . cv_df.groupby(&#39;model_name&#39;).accuracy.mean() . model_name LinearSVC 0.660002 LogisticRegression 0.645332 MultinomialNB 0.584395 RandomForestClassifier 0.381590 Name: accuracy, dtype: float64 . LinearSVC and Logistic Regression perform better than the other two classifiers, with LinearSVC having a slight advantage with a median accuracy of around 66%. Focusing on the LinearSVC model, which has demonstrated to perform the best, I report its confusing matrix, to show the discrepancies between predicted and actual labels. . model = LinearSVC() X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0) model.fit(X_train, y_train) y_pred = model.predict(X_test) from sklearn.metrics import confusion_matrix conf_mat = confusion_matrix(y_test, y_pred) fig, ax = plt.subplots(figsize=(10,10)) sns.heatmap(conf_mat, annot=True, fmt=&#39;d&#39;, xticklabels=category_id_df.cpc.values, yticklabels=category_id_df.cpc.values) plt.ylabel(&#39;Actual&#39;) plt.xlabel(&#39;Predicted&#39;) plt.show() .",
            "url": "https://themuahdibportfolio.github.io/Blog/nlp/network%20analysis/intellectual%20property/patent%20data/2022/02/08/_02_08_NLP.html",
            "relUrl": "/nlp/network%20analysis/intellectual%20property/patent%20data/2022/02/08/_02_08_NLP.html",
            "date": " ‚Ä¢ Feb 8, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://themuahdibportfolio.github.io/Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Edoardo is a master‚Äôs student at Copenhagen Business School. His major is in Applied Economics and Finance, whereas his minor is in data science. .",
          "url": "https://themuahdibportfolio.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://themuahdibportfolio.github.io/Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}