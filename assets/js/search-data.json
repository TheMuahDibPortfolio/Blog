{
  
    
        "post0": {
            "title": "Topic modelling on patent main claims",
            "content": "Import of packages . import pandas as pd from io import StringIO import matplotlib.pyplot as plt import numpy as np import seaborn as sns %matplotlib inline sns.set(color_codes=True) . Load the data . The data are retrieved from PatBase. I select publications with a specific legal status, e.g., granted publications, and from a unique jurisdiction, such as the USA&#39;s one. . df = pd.read_csv(&#39;/content/drive/MyDrive/Colab Notebooks/DSBA Patent Project /Paper /EDA &amp; NLP /patbase_export_274777127.csv&#39;) . col = [&#39;Cooperative Patent Class&#39;, &#39;1st Main Claim&#39;] df = df[col] df = df.dropna() . df[&#39;cpc&#39;] = df[&#39;Cooperative Patent Class&#39;].str.extract(r&#39;(^.{0,1})&#39;) . Text cleaning . df[&#39;main_claim&#39;] = df[&#39;1st Main Claim&#39;].str.replace(&#39; [EN ] s1. s&#39;, &#39;&#39;, regex = True) # string stripping EN . df[&#39;main_claim&#39;] = df[&#39;main_claim&#39;].str.lower() . Tokenization . Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens are words. . Modules to tokenize . import nltk nltk.download(&#39;stopwords&#39;) nltk.download(&#39;wordnet&#39;) from nltk.stem import WordNetLemmatizer from nltk.corpus import stopwords english_stops = set(stopwords.words(&#39;english&#39;)) . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Unzipping corpora/stopwords.zip. [nltk_data] Downloading package wordnet to /root/nltk_data... [nltk_data] Unzipping corpora/wordnet.zip. . df.index = range(len(df)) . Using spaCy to tokenize . spaCy is a free open-source library for Natural Language Processing in Python. . import spacy nlp = spacy.load(&quot;en&quot;) . tokens = [] # iterate SpaCy preprocessing throughout all the main claims for claim in nlp.pipe(df[&#39;main_claim&#39;]): proj_tok = [token.lemma_.lower() for token in claim if token.pos_ in [&#39;NOUN&#39;, &#39;PROPN&#39;, &#39;ADJ&#39;, &#39;ADV&#39;] and not token.is_stop] tokens.append(proj_tok) # it takes 11 minutes . df[&#39;tokens&#39;] = tokens . Create a corpus with Gensim . Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community. . from gensim.corpora.dictionary import Dictionary . dictionary = Dictionary(df[&#39;tokens&#39;]) . dictionary.filter_extremes(no_below=5, no_above=0.5) . corpus = [dictionary.doc2bow(doc) for doc in df[&#39;tokens&#39;]] . # Create Dictionary import gensim.corpora as corpora id2word = corpora.Dictionary(df[&#39;tokens&#39;]) [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]] . [[(&#39;air&#39;, 9), (&#39;angle&#39;, 1), (&#39;chassis&#39;, 1), (&#39;computer&#39;, 1), (&#39;direct&#39;, 2), (&#39;direction&#39;, 4), (&#39;direction;a&#39;, 2), (&#39;fan&#39;, 4), (&#39;flow&#39;, 6), (&#39;housing&#39;, 6), (&#39;oblique&#39;, 2), (&#39;parallel&#39;, 1), (&#39;partially&#39;, 1), (&#39;radiator&#39;, 4), (&#39;relative&#39;, 1), (&#39;second&#39;, 2), (&#39;system&#39;, 1)]] . LDA . LDA is a probabilistic topic modelling method that identifies abstract themes from a set of text documents. LDA makes two assumptions: . it assumes that documents with similar topics will use similar groups of words. | it assumes that each document is a mix of multiple topics. Moreover, each topic is described by a set of words. | from gensim.models import LdaMulticore . Finding the optimal number of topic . The approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value. The optimal number of &#39;k&#39; is the one that appears first after a fast growth in the coherence value. A causality of &#39;k&#39; being too large is that you see the same words repeated across multiple topics. . import gensim from gensim.models import CoherenceModel . def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3): &quot;&quot;&quot; Compute c_v coherence for various number of topics Parameters: - dictionary : Gensim dictionary corpus : Gensim corpus texts : List of input texts limit : Max num of topics Returns: - model_list : List of LDA topic models coherence_values : Coherence values corresponding to the LDA model with respective number of topics &quot;&quot;&quot; coherence_values = [] model_list = [] for num_topics in range(start, limit, step): model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word) model_list.append(model) coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence=&#39;c_v&#39;) coherence_values.append(coherencemodel.get_coherence()) return model_list, coherence_values . model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts= df[&#39;tokens&#39;], start=2, limit=40, step=6) . limit=40; start=2; step=6; x = range(start, limit, step) plt.plot(x, coherence_values) plt.xlabel(&quot;Num Topics&quot;) plt.ylabel(&quot;Coherence score&quot;) plt.legend((&quot;coherence_values&quot;), loc=&#39;best&#39;) plt.show() . Coherence Scores . for m, cv in zip(x, coherence_values): print(&quot;Num Topics =&quot;, m, &quot; has Coherence Value of&quot;, round(cv, 4)) . Num Topics = 2 has Coherence Value of 0.4005 Num Topics = 8 has Coherence Value of 0.4206 Num Topics = 14 has Coherence Value of 0.45 Num Topics = 20 has Coherence Value of 0.4517 Num Topics = 26 has Coherence Value of 0.4612 Num Topics = 32 has Coherence Value of 0.4503 Num Topics = 38 has Coherence Value of 0.4867 . print(model_list[2]) . LdaModel(num_terms=57990, num_topics=14, decay=0.5, chunksize=2000) . from pprint import pprint . Visualization . !pip install -qq pyLDAvis . !pip install -qq pyLDAvis . import pyLDAvis . import pyLDAvis.gensim_models %matplotlib inline pyLDAvis.enable_notebook() . lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=14) . lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary) . /usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only by=&#39;saliency&#39;, ascending=False).head(R).drop(&#39;saliency&#39;, 1) . pyLDAvis.display(lda_display) . The above visualization shows the 15 topics that LDA detached. You can select the topic on the top left-hand side corner. Consequently, the bar chart on the right-hand side will show the Top-30 Most Relevant Terms for topic. Moreover, you can adjust the value of lambda Œª: when Œª decreases, the bar chart shows words that have a higher ratio of red to grey, i.e., the frequency given the topic to the overall frequency of the word in the whole dictionary. This way it is much easier to infer the topic. . Inferring the topics . memory | tubular container | metal conductor | electronic circuit | mobile device | frame | video chat | text document | laser | system for a liquid | virtual storage | turbine | antibody | rotor |",
            "url": "https://themuahdibportfolio.github.io/Blog/nlp/text%20cleaning/text%20tokenization/corpus%20creation/topic%20modelling/lda%20optmiation/topic%20visualization/topic%20inference/intellectual%20property/patent%20data/2022/03/02/_02_18_TopicModel.html",
            "relUrl": "/nlp/text%20cleaning/text%20tokenization/corpus%20creation/topic%20modelling/lda%20optmiation/topic%20visualization/topic%20inference/intellectual%20property/patent%20data/2022/03/02/_02_18_TopicModel.html",
            "date": " ‚Ä¢ Mar 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Predict the Cooperative Patent Class (CPC) by means of NLP",
            "content": "Import of packages . import pandas as pd # data analysis from io import StringIO import matplotlib.pyplot as plt import numpy as np import seaborn as sns %matplotlib inline sns.set(color_codes=True) ## Load the data df = pd.read_csv(&#39;/content/drive/MyDrive/Colab Notebooks/DSBA Patent Project /Paper /EDA &amp; NLP /patbase_export_274777127.csv&#39;) col = [&#39;Cooperative Patent Class&#39;, &#39;1st Main Claim&#39;] #we only use those two columns df = df[col] df = df.dropna() . The data source of the patent-level data is PatBase, that is a product offered by MineSoft, i.e., patent solutions provider founded in 1996 offering online products and services, such as, patent research, monitoring, and analysis, as well as other intellectual property services. Moreover, the only two features needed in this prediction exercise are the Cooperative Patent Class and the text of the 1&#39; Main Claim. . Making CPC label . We can see that each patent publication, i.e., each row, has multiple CPC labels. For convinience, we only use the first label in order of appearance. . df[&#39;cpc&#39;] = df[&#39;Cooperative Patent Class&#39;].str.extract(r&#39;(^.{0,1})&#39;) . RegEx preprocessing . df[&#39;main_claim&#39;] = df[&#39;1st Main Claim&#39;].str.replace(&#39; [EN ] s1. s&#39;, &#39;&#39;, regex = True) # string stripping EN df[&#39;main_claim&#39;] = df[&#39;main_claim&#39;].str.lower() df[&#39;main_claim&#39;] = df[&#39;main_claim&#39;].str.replace(&#39; d+&#39;, &#39;&#39;) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version. &#34;&#34;&#34; . Building word count vectors with scikit-learn . from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split . y = df[&#39;cpc&#39;] . X_train, X_test, y_train, y_test = train_test_split(df[&#39;main_claim&#39;],y,test_size=0.33,random_state=53) . CountVectorizer for text classification . count_vectorizer = CountVectorizer(stop_words=&#39;english&#39;) # Transform the training data using only the &#39;text&#39; column values: count_train count_train = count_vectorizer.fit_transform(X_train) # Transform the test data using only the &#39;text&#39; column values: count_test count_test = count_vectorizer.transform(X_test) . TfidfVectorizer for text classification . from sklearn.feature_extraction.text import TfidfVectorizer . tfidf_vectorizer = TfidfVectorizer(stop_words=&#39;english&#39;, sublinear_tf= True, min_df=5, ngram_range= (1,2), norm=&#39;l2&#39;, encoding=&#39;latin-1&#39;) # Transform the training data: tfidf_train tfidf_train = tfidf_vectorizer.fit_transform(X_train) # Transform the test data: tfidf_test tfidf_test = tfidf_vectorizer.transform(X_test) . Naive Bayes: Text classification model . Training and testing the labelling model with CountVectorizer . from sklearn.naive_bayes import MultinomialNB from sklearn import metrics # Instantiate a Multinomial Naive Bayes classifier: nb_classifier nb_classifier = MultinomialNB(alpha = 2.6) # Fit the classifier to the training data nb_classifier.fit(count_train, y_train) # Create the predicted tags: pred predct = nb_classifier.predict(count_test) . score = metrics.accuracy_score(y_test,predct) print(score) . 0.6304475278483487 . Training and testing the labelling model with TfidfVectorizer . nb_classifier = MultinomialNB() # Fit the classifier to the training data nb_classifier.fit(tfidf_train, y_train) # Create the predicted tags: pred predtf = nb_classifier.predict(tfidf_test) . score = metrics.accuracy_score(y_test,predtf) print(score) . Attempts at improving the model . Experimenting with different alphas parameters for the Multinomial Naive Bayes model. . alphas = np.arange(2,3,0.1) # Define train_and_predict() def train_and_predict(alpha): # Instantiate the classifier: nb_classifier nb_classifier = MultinomialNB(alpha=alpha) # Fit to the training data nb_classifier.fit(count_train,y_train) # Predict the labels: pred pred = nb_classifier.predict(count_test) # Compute accuracy: score score = metrics.accuracy_score(y_test,pred) return score # Iterate over the alphas and print the corresponding score for alpha in alphas: print(&#39;Alpha: &#39;, alpha) print(&#39;Score: &#39;, train_and_predict(alpha)) print() . Alpha: 2.0 Score: 0.6294703928082861 Alpha: 2.1 Score: 0.6294703928082861 Alpha: 2.2 Score: 0.6296658198162987 Alpha: 2.3000000000000003 Score: 0.6302521008403361 Alpha: 2.4000000000000004 Score: 0.6306429548563611 Alpha: 2.5000000000000004 Score: 0.6306429548563611 Alpha: 2.6000000000000005 Score: 0.6304475278483487 Alpha: 2.7000000000000006 Score: 0.6294703928082861 Alpha: 2.8000000000000007 Score: 0.6300566738323237 Alpha: 2.900000000000001 Score: 0.6290795387922611 . Inspecting count vectorizing Multinomial Naive Bayes model . The result of the inspection is not clear to me. Indeed, all the classes&#39; features are the same except for the first class. . class_labels = nb_classifier.classes_ . feature_names = count_vectorizer.get_feature_names() . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead. warnings.warn(msg, category=FutureWarning) . feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names)) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:103: FutureWarning: Attribute `coef_` was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26). warnings.warn(msg, category=FutureWarning) . print(class_labels[0], feat_with_weights[:20]) . A [(-12.073243863160105, &#39;_n&#39;), (-12.073243863160105, &#39;aabb&#39;), (-12.073243863160105, &#39;aabbco&#39;), (-12.073243863160105, &#39;aabbs&#39;), (-12.073243863160105, &#39;aacmm&#39;), (-12.073243863160105, &#39;aad&#39;), (-12.073243863160105, &#39;abandon&#39;), (-12.073243863160105, &#39;abatement&#39;), (-12.073243863160105, &#39;abbe&#39;), (-12.073243863160105, &#39;aberration&#39;), (-12.073243863160105, &#39;abiotically&#39;), (-12.073243863160105, &#39;abl&#39;), (-12.073243863160105, &#39;ablating&#39;), (-12.073243863160105, &#39;ablative&#39;), (-12.073243863160105, &#39;abnormalities&#39;), (-12.073243863160105, &#39;abnormality&#39;), (-12.073243863160105, &#39;abnormally&#39;), (-12.073243863160105, &#39;abnormity&#39;), (-12.073243863160105, &#39;abort&#39;), (-12.073243863160105, &#39;aborted&#39;)] . print(class_labels[1], feat_with_weights[-20:]) . B [(-5.706773415428667, &#39;including&#39;), (-5.668015405129263, &#39;coupled&#39;), (-5.654878927223893, &#39;base&#39;), (-5.632297322527184, &#39;assembly&#39;), (-5.5704538172444815, &#39;extending&#39;), (-5.516465507002063, &#39;anda&#39;), (-5.513628625666863, &#39;position&#39;), (-5.396160401912969, &#39;user&#39;), (-5.247783826904798, &#39;body&#39;), (-5.230560580921683, &#39;member&#39;), (-5.103453193258515, &#39;device&#39;), (-4.969921800633992, &#39;plurality&#39;), (-4.92175839925537, &#39;surface&#39;), (-4.863164234989317, &#39;having&#39;), (-4.716325620804084, &#39;configured&#39;), (-4.604730591663768, &#39;end&#39;), (-4.5411557196183825, &#39;portion&#39;), (-4.397233931131217, &#39;comprising&#39;), (-4.065543850276079, &#39;said&#39;), (-3.9583208889555124, &#39;second&#39;)] . print(class_labels) . [&#39;A&#39; &#39;B&#39; &#39;C&#39; &#39;D&#39; &#39;E&#39; &#39;F&#39; &#39;G&#39; &#39;H&#39;] . print(class_labels[6], feat_with_weights[-20:]) . G [(-5.706773415428667, &#39;including&#39;), (-5.668015405129263, &#39;coupled&#39;), (-5.654878927223893, &#39;base&#39;), (-5.632297322527184, &#39;assembly&#39;), (-5.5704538172444815, &#39;extending&#39;), (-5.516465507002063, &#39;anda&#39;), (-5.513628625666863, &#39;position&#39;), (-5.396160401912969, &#39;user&#39;), (-5.247783826904798, &#39;body&#39;), (-5.230560580921683, &#39;member&#39;), (-5.103453193258515, &#39;device&#39;), (-4.969921800633992, &#39;plurality&#39;), (-4.92175839925537, &#39;surface&#39;), (-4.863164234989317, &#39;having&#39;), (-4.716325620804084, &#39;configured&#39;), (-4.604730591663768, &#39;end&#39;), (-4.5411557196183825, &#39;portion&#39;), (-4.397233931131217, &#39;comprising&#39;), (-4.065543850276079, &#39;said&#39;), (-3.9583208889555124, &#39;second&#39;)] . Selection of various model with count vectorizing . from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.svm import LinearSVC from sklearn.model_selection import cross_val_score . models = [ RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0), LinearSVC(), MultinomialNB(alpha = 2.6), LogisticRegression(random_state=0), ] . CV = 5 cv_df = pd.DataFrame(index=range(CV * len(models))) entries = [] for model in models: model_name = model.__class__.__name__ accuracies = cross_val_score(model, count_train, y_train, scoring=&#39;accuracy&#39;, cv=CV) for fold_idx, accuracy in enumerate(accuracies): entries.append((model_name, fold_idx, accuracy)) cv_df = pd.DataFrame(entries, columns=[&#39;model_name&#39;, &#39;fold_idx&#39;, &#39;accuracy&#39;]) . cv_df.groupby(&#39;model_name&#39;).accuracy.mean() . model_name LinearSVC 0.574743 LogisticRegression 0.610743 MultinomialNB 0.625468 RandomForestClassifier 0.374146 Name: accuracy, dtype: float64 . Selection of various model with tfidf vectorizing . CV = 5 cv_df = pd.DataFrame(index=range(CV * len(models))) entries = [] for model in models: model_name = model.__class__.__name__ accuracies = cross_val_score(model, tfidf_train, y_train, scoring=&#39;accuracy&#39;, cv=CV) for fold_idx, accuracy in enumerate(accuracies): entries.append((model_name, fold_idx, accuracy)) cv_df = pd.DataFrame(entries, columns=[&#39;model_name&#39;, &#39;fold_idx&#39;, &#39;accuracy&#39;]) . cv_df.groupby(&#39;model_name&#39;).accuracy.mean() . model_name LinearSVC 0.674175 LogisticRegression 0.658003 MultinomialNB 0.547213 RandomForestClassifier 0.374916 Name: accuracy, dtype: float64 . LinearSVC and Logistic Regression perform better than the other two classifiers, with LinearSVC having a slight advantage with a median accuracy of 67%. Focusing on the LinearSVC model, which has demonstrated to perform the best, I report its confusing matrix, to show the discrepancies between predicted and actual labels. .",
            "url": "https://themuahdibportfolio.github.io/Blog/nlp/multi-class%20text%20classification/intellectual%20property/patent%20data/text%20vectorization%20models/bag%20of%20words%20model/multinomial%20naive%20bayes%20model/multi-model%20selection2022/03/02/_02_18_MultiClassPatent.html",
            "relUrl": "/nlp/multi-class%20text%20classification/intellectual%20property/patent%20data/text%20vectorization%20models/bag%20of%20words%20model/multinomial%20naive%20bayes%20model/multi-model%20selection2022/03/02/_02_18_MultiClassPatent.html",
            "date": " ‚Ä¢ Mar 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Patent network analysis",
            "content": "import pandas as pd import string as str import regex as re from google.colab import drive drive.mount(&#39;/content/drive&#39;) . df = pd.read_csv(&#39;/content/drive/MyDrive/Colab Notebooks/DSBA Patent Project /Paper /test NPA/patbase_export_296387709.csv&#39;) . EDA . df.columns . Index([&#39;Family Number&#39;, &#39;Earliest Priority Date&#39;, &#39;Extended Family Number&#39;, &#39;Calculated Expiry Date&#39;, &#39;Earliest Priority Country&#39;, &#39;Original Patent Number&#39;, &#39;Original Kind Code&#39;, &#39;Original Application Number&#39;, &#39;DOCDB Patent Number&#39;, &#39;DOCDB Kind Code&#39;, &#39;DOCDB Application Number&#39;, &#39;Earliest Publication Number&#39;, &#39;Earliest Publication Date&#39;, &#39;Assignees&#39;, &#39;Inventors&#39;, &#39;Inventor Details Name&#39;, &#39;Inventor Details Std/Non Std&#39;, &#39;Inventor Details Address&#39;, &#39;Assignee Details Name&#39;, &#39;Assignee Details Std/Non Std&#39;, &#39;Assignee Details Address&#39;, &#39;Agent&#39;, &#39;Agent Details Name&#39;, &#39;Agent Details Address&#39;, &#39;Title&#39;, &#39;Abstract&#39;, &#39;1st Main Claim&#39;, &#39;Number of Claims&#39;, &#39;Number of Independent Claims&#39;, &#39;Backward Citations&#39;, &#39;Forward Citations&#39;, &#39;Number of Backward Citations&#39;, &#39;Number of Forward Citations&#39;, &#39;NPL Citations&#39;, &#39;Number of NPL Citations&#39;, &#39;Cooperative Patent Class&#39;, &#39;International Class&#39;, &#39;US Class&#39;, &#39;Patent Type&#39;, &#39;Grant&#39;], dtype=&#39;object&#39;) . print(df.shape) . (3762, 40) . df.head(3) . Family Number Earliest Priority Date Extended Family Number Calculated Expiry Date Earliest Priority Country Original Patent Number Original Kind Code Original Application Number DOCDB Patent Number DOCDB Kind Code DOCDB Application Number Earliest Publication Number Earliest Publication Date Assignees Inventors Inventor Details Name Inventor Details Std/Non Std Inventor Details Address Assignee Details Name Assignee Details Std/Non Std Assignee Details Address Agent Agent Details Name Agent Details Address Title Abstract 1st Main Claim Number of Claims Number of Independent Claims Backward Citations Forward Citations Number of Backward Citations Number of Forward Citations NPL Citations Number of NPL Citations Cooperative Patent Class International Class US Class Patent Type Grant . 0 20652658 | 19961031 | NaN | 20171031.0 | DK | US06020349 | A | US08962098 | US6020349 | A | US96209897 | US6020349 A | 20000201 | NOVO NORDISK AS | ANKERSEN MICHAEL; STIDSEN CARSTEN ENGGAARD; CR... | ANKERSEN MICHAEL; STIDSEN CARSTEN ENGGAARD; CR... | Standard; Standard; Standard; Standard | DK; DK; US; DK | NOVO NORDISK AS; NOVO NORDISK A S | Standard; Non Standard | DK; | NaN | NaN | NaN | [EN] CONSTRAINED SOMATOSTATIN AGONISTS AND ANT... | [EN] The present invention relates to a compou... | [EN] 1. A compound of formula I: wherein A is ... | 20 | 1 | DE3631334 A1; EP0304330 A1; EP0448765 A1; US42... | US2003195187 AA; US2003207814 AA; US2003229025... | 6 | 40 | Woderer et al., Chem. Ber. 119, 2050-2054, (19... | 2 | A61P1/00; A61P11/00; A61P25/00; A61P27/02; A61... | A61K31/44; A61K31/4439; A61K31/445; A61K31/454... | 514/253.01; 514/318; 514/340; 514/341; 514/866... | NaN | YES | . 1 20652658 | 19961031 | NaN | 20171031.0 | DK | US06083960 | A | US09397355 | US6083960 | A | US39735599 | US6083960 A | 20000704 | NOVO NORDISK AS | ANKERSEN MICHAEL; STIDSEN CARSTEN ENGGAARD; CR... | ANKERSEN MICHAEL; STIDSEN CARSTEN ENGGAARD; CR... | Standard; Standard; Standard; Standard | DK; DK; US; DK | NOVO NORDISK AS; NOVO NORDISK A S | Standard; Non Standard | DK; | NaN | NaN | NaN | [EN] CONSTRAINED SOMATOSTATIN AGONISTS AND ANT... | [EN] The present invention relates to a compou... | [EN] 1. A compound formula I wherein A is pyri... | 11 | 1 | DE3631334 A1; EP0304330 A1; EP0448765 A1; US42... | US2004106625 AA; US2008153835 AA; US7279493 BB... | 6 | 4 | Woderer et al., Chem. Ber. 119, 2050-2054, (19... | 2 | A61P1/00; A61P11/00; A61P25/00; A61P27/02; A61... | A61K31/44; A61K31/4439; A61K31/445; A61K31/454... | 514/253.01; 514/318; 514/338; 514/344; 514/866... | NaN | YES | . 2 15295978 | 19990817 | NaN | 20190826.0 | DK | US06586574 | B1 | US09638590 | US6586574 | B1 | US63859000 | US6586574 BA | 20030701 | NN AS | HANSEN LARS LINDGAARD | HANSEN LARS LINDGAARD; LARS LINDGAARD HANSEN | Standard; Non Standard | DK; GADSTRUP DK | NN AS; NN A S; NOVO NORDISK A S; NOVO NORDISK ... | Standard; Non Standard; Non Standard; Non Stan... | DK; BAGSVAERD DK; ; ; | MARC A BEGAN ESQ; REZA GREEN ESQ; RICHARD W BO... | MARC A BEGAN ESQ; REZA GREEN ESQ; RICHARD W BO... | ; ; | [EN] STABILIZATION OF FREEZE-DRIED CAKE | [EN] The invention relates to the use of glycy... | [EN] 1. A lyophilized composition comprising a... | 13 | 2 | EP0359201 A2; US3773626 A; US4297344 A; US4687... | CN107073082 A; EP3182993 A4; EP3750556 A1; US1... | 9 | 44 | Davis et al., Archives of Biochemistry and Bio... | 5 | A61K47/183; A61K47/26; A61K9/19; C12N9/96 | A61K47/16; A61K47/18; A61K47/26; A61K9/19; C12... | 252/363.5; 530/384; 514/971; 514/834; 514/802;... | NaN | YES | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; print(df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 3762 entries, 0 to 3761 Data columns (total 40 columns): # Column Non-Null Count Dtype -- -- 0 Family Number 3762 non-null int64 1 Earliest Priority Date 3762 non-null int64 2 Extended Family Number 154 non-null float64 3 Calculated Expiry Date 2810 non-null float64 4 Earliest Priority Country 3762 non-null object 5 Original Patent Number 3737 non-null object 6 Original Kind Code 3737 non-null object 7 Original Application Number 3737 non-null object 8 DOCDB Patent Number 3748 non-null object 9 DOCDB Kind Code 3748 non-null object 10 DOCDB Application Number 3748 non-null object 11 Earliest Publication Number 3762 non-null object 12 Earliest Publication Date 3762 non-null int64 13 Assignees 3586 non-null object 14 Inventors 3747 non-null object 15 Inventor Details Name 3761 non-null object 16 Inventor Details Std/Non Std 3761 non-null object 17 Inventor Details Address 3760 non-null object 18 Assignee Details Name 3762 non-null object 19 Assignee Details Std/Non Std 3762 non-null object 20 Assignee Details Address 3747 non-null object 21 Agent 1887 non-null object 22 Agent Details Name 1887 non-null object 23 Agent Details Address 882 non-null object 24 Title 3749 non-null object 25 Abstract 3743 non-null object 26 1st Main Claim 3747 non-null object 27 Number of Claims 3762 non-null int64 28 Number of Independent Claims 3762 non-null int64 29 Backward Citations 3351 non-null object 30 Forward Citations 3006 non-null object 31 Number of Backward Citations 3762 non-null int64 32 Number of Forward Citations 3762 non-null int64 33 NPL Citations 2328 non-null object 34 Number of NPL Citations 3762 non-null int64 35 Cooperative Patent Class 3733 non-null object 36 International Class 3746 non-null object 37 US Class 3463 non-null object 38 Patent Type 17 non-null object 39 Grant 3762 non-null object dtypes: float64(2), int64(8), object(30) memory usage: 1.1+ MB None . There are three variables that relate to the inventor-level, being . The inventor | Inventor Details Name | Inventor Details Address | . df[&#39;DOCDB Patent Number&#39;].nunique() # we could use this variable as a unique ID . 3748 . By means of EDA, we found out that we could use the above variable as a unique ID variable. . print(df.Inventors.head(1)) . 0 ANKERSEN MICHAEL; STIDSEN CARSTEN ENGGAARD; CR... Name: Inventors, dtype: object . Create the data set for the network . df1 = df.Inventors.str.split(&#39;;&#39;, expand = True) # creating a wide data set from the invetors column, placing each and every inventor in a different column df1.head(2) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . 1 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df1 = df1.join(df[&#39;DOCDB Patent Number&#39;]) # join also the column of unique IDs # df1 = df1.join(df[&#39;Inventor Details Address&#39;]) for the future df1.head(2) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 DOCDB Patent Number . 0 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6020349 | . 1 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6083960 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df1.head(10) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 DOCDB Patent Number . 0 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6020349 | . 1 ANKERSEN MICHAEL | STIDSEN CARSTEN ENGGAARD | CRIDER ALBERT MICHAEL | DORWALD FLORENZIO ZARAGOZA | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6083960 | . 2 HANSEN LARS LINDGAARD | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6586574 | . 3 KOCH KAREN | KVORNING INGELISE | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US7018992 | . 4 HOFSTAETTER THIBAUD | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US5860946 | . 5 ROJKJAER RASMUS | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US2003118580 | . 6 ROJKJAER RASMUS | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US2008075709 | . 7 AHMADIAN HALEH | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US2008268521 | . 8 PERSSON EGON | OLSEN OLE HVILSTED | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6911323 | . 9 SORENSEN BRIT BINOW | PETERSEN LARS CHRISTIAN | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | None | US6858587 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df1[&#39;DOCDB Patent Number&#39;].isnull().sum() . 14 . df1 = df1.dropna(subset=[&#39;DOCDB Patent Number&#39;]) df1.shape . (3748, 19) . new_col_list = [&#39;inventor1&#39;, &#39;inventor2&#39;, &#39;inventor3&#39;, &#39;inventor4&#39;, &#39;inventor5&#39;, &#39;inventor6&#39;, &#39;inventor7&#39;, &#39;inventor8&#39;, &#39;inventor9&#39;, &#39;inventor10&#39;, &#39;inventor11&#39;, &#39;inventor12&#39;, &#39;inventor13&#39;, &#39;inventor14&#39;, &#39;inventor15&#39;, &#39;inventor16&#39;, &#39;inventor17&#39;, &#39;inventor18&#39;, &#39;patent_number&#39;] . old_columns = list(df1) print(old_columns) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, &#39;DOCDB Patent Number&#39;] . df1.rename(columns = {old_columns[idx]: name for (idx, name) in enumerate (new_col_list)}, inplace = True) . df2 = pd.wide_to_long(df1, stubnames=&#39;inventor&#39;,i = &#39;patent_number&#39;, j = &#39;irrelevant&#39;) . df2 = df2.dropna() . df2.head(3) # here we can appreciate a dataframe where the inventor is indexed by the patent number . inventor . patent_number irrelevant . US6020349 1 ANKERSEN MICHAEL | . US6083960 1 ANKERSEN MICHAEL | . US6586574 1 HANSEN LARS LINDGAARD | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df2.shape # indeed the shape of such a dataframe is given by the product of the number of unique inventors times the time that they invented . (13253, 1) . df2 = df2.reset_index(level=[&#39;patent_number&#39;]) #reset_index transforms an index of a df into a features . df2.head() . patent_number inventor . irrelevant . 1 US6020349 | ANKERSEN MICHAEL | . 1 US6083960 | ANKERSEN MICHAEL | . 1 US6586574 | HANSEN LARS LINDGAARD | . 1 US7018992 | KOCH KAREN | . 1 US5860946 | HOFSTAETTER THIBAUD | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df2[&#39;inventor_id&#39;] = df2[&#39;inventor&#39;].factorize()[0] #create a new column with a id per each and every unique inventor . df2.head(6) . patent_number inventor inventor_id . irrelevant . 1 US6020349 | ANKERSEN MICHAEL | 0 | . 1 US6083960 | ANKERSEN MICHAEL | 0 | . 1 US6586574 | HANSEN LARS LINDGAARD | 1 | . 1 US7018992 | KOCH KAREN | 2 | . 1 US5860946 | HOFSTAETTER THIBAUD | 3 | . 1 US2003118580 | ROJKJAER RASMUS | 4 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df2[&#39;patent_number&#39;].nunique() # this number is 1 less than some cells before . 3747 . Edge list . We can create an edge list merging the dataframe with itself using the patent_number, so that, each and every row represents the coinvention of two inventors of the same patent. The only thing that we then need to remove are self-links since a person can not really invent with herself. . edges = pd.merge(df2, df2, on = &#39;patent_number&#39;) . edges.head(2) # talking about self-links, you have an example just here . patent_number inventor_x inventor_id_x inventor_y inventor_id_y . 0 US6020349 | ANKERSEN MICHAEL | 0 | ANKERSEN MICHAEL | 0 | . 1 US6020349 | ANKERSEN MICHAEL | 0 | STIDSEN CARSTEN ENGGAARD | 1487 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; edges = edges[edges.inventor_id_x != edges.inventor_id_y] . . We are now in a situation where people that co-invented a patent will have at least one row per invention. This can be aggregated in the following way by grouping. . # How many times did Person X and Person Y co-invented together # reset_index makes everytging from a multi-index-series into a dataframe edges = edges.groupby([&#39;inventor_id_x&#39;, &#39;inventor_id_y&#39;]).size().reset_index() . edges.head(2) . inventor_id_x inventor_id_y 0 . 0 0 | 1487 | 3 | . 1 0 | 1520 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; The column called &quot;0&quot; counts how many time a person x &amp; a person y co-invent . edges.shape . (29532, 3) . import os import matplotlib.pyplot as plt import seaborn as sns . dist_coinv = edges[0].value_counts() # distribution of the number of co-inventions, i.e., there are 5270 pair of inventors that have co-invented 2 patents. dist_coinv = dist_coinv[:7, ] plt.figure(figsize = (10,5)) sns.barplot(dist_coinv.index, dist_coinv.values, alpha = 0.8) plt.title(&#39;Distribution of the occurrences of co-inventing&#39;) plt.ylabel(&#39;Number of occurrences&#39;, fontsize=12) plt.xlabel(&#39;Number of co-invention&#39;, fontsize=12) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . edges.rename({0:&#39;weight&#39;}, axis = 1, inplace=True) . Creating the Graph object with NetworkX . Now we can create a network object from this edgelist. . import networkx as nx from community import community_louvain . # The nodes are the inventors and the edges are weighted by the number of co-invention occured between two inventors G = nx.from_pandas_edgelist(edges, source=&#39;inventor_id_x&#39;, target=&#39;inventor_id_y&#39;, edge_attr=&#39;weight&#39;, create_using = nx.Graph()) . df2[[&#39;inventor_id&#39;,&#39;inventor&#39;]] . inventor_id inventor . irrelevant . 1 0 | ANKERSEN MICHAEL | . 1 0 | ANKERSEN MICHAEL | . 1 1 | HANSEN LARS LINDGAARD | . 1 2 | KOCH KAREN | . 1 3 | HOFSTAETTER THIBAUD | . ... ... | ... | . 15 4135 | KUKI ATSUO | . 16 4136 | VANOVA HANA | . 16 4137 | TESTON KIMBERLY ANN | . 17 4129 | TENG MIN | . 18 4134 | KIEL DAN | . 13253 rows √ó 2 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df2[[&#39;inventor_id&#39;,&#39;inventor&#39;]].set_index(&#39;inventor_id&#39;) . inventor . inventor_id . 0 ANKERSEN MICHAEL | . 0 ANKERSEN MICHAEL | . 1 HANSEN LARS LINDGAARD | . 2 KOCH KAREN | . 3 HOFSTAETTER THIBAUD | . ... ... | . 4135 KUKI ATSUO | . 4136 VANOVA HANA | . 4137 TESTON KIMBERLY ANN | . 4129 TENG MIN | . 4134 KIEL DAN | . 13253 rows √ó 1 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; node_attributes = df2[[&#39;inventor_id&#39;,&#39;inventor&#39;]].set_index(&#39;inventor_id&#39;).drop_duplicates().to_dict(&#39;index&#39;) . nx.set_node_attributes(G, {G.degree(): &#39;degree&#39;}) . nx.set_node_attributes(G, node_attributes) . len(G.nodes()) . 3961 . len(G.edges()) . 14766 . Centrality indicators . From here we will calculate various centrality measures and perform community detection. Think about the latter as UML (which it actually is). This will allow us to investigate e.g.: . Who are the top people in these communities? | . centrality_dgr = nx.degree_centrality(G) centrality_eig = nx.eigenvector_centrality_numpy(G) partition = community_louvain.best_partition(G) #that will take some time... degree = G.degree() . nx.set_node_attributes(G, centrality_dgr, &#39;dgr&#39;) nx.set_node_attributes(G, centrality_eig, &#39;eig&#39;) nx.set_node_attributes(G, partition, &#39;partition&#39;) nx.set_node_attributes(G, dict(degree), &#39;degree_basic&#39;) . Bringing it back to pandas . Once all graph indicators are in place, we can bring them back to Pandas for easier further analysis üßê. You can compare that step to inspecting individual clusters identified with e.g. K-means. . nodes_df = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient=&#39;index&#39;) . nodes_df.sort_values(&#39;eig&#39;, ascending=False)[:10] . inventor dgr eig partition degree_basic . 1655 LAU JESPER | 0.047727 | 0.322390 | 48 | 189 | . 1558 MADSEN PETER | 0.038636 | 0.223897 | 11 | 153 | . 1762 KODRA JANOS TIBOR | 0.025505 | 0.207342 | 48 | 101 | . 1763 BEHRENS CARSTEN | 0.024495 | 0.165824 | 48 | 97 | . 2359 GARIBAY PATRICK WILLIAM | 0.017424 | 0.138849 | 6 | 69 | . 2052 THOEGERSEN HENNING | 0.022475 | 0.135706 | 6 | 89 | . 2489 BLOCH PAW | 0.014646 | 0.131985 | 48 | 58 | . 1630 ANDERSEN HENRIK SUNE | 0.024495 | 0.127523 | 11 | 97 | . 1519 ANKERSEN MICHAEL | 0.015657 | 0.127359 | 48 | 62 | . 1823 MADSEN KJELD | 0.018687 | 0.122319 | 48 | 74 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; nodes_df.partition.nunique() . 284 . dist_part = nodes_df.partition.value_counts()[:10] plt.figure(figsize=(10,5)) sns.barplot(dist_part.index, dist_part.values, alpha=0.8) plt.title(&#39;Distribution of inventors amoing the top largest 10 partitions&#39;) plt.ylabel(&#39;Number of Occurrences&#39;, fontsize=12) plt.xlabel(&#39;Partitions&#39;, fontsize=12) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . top5_part = nodes_df.partition.value_counts()[:5].index . top5_part_nodes = nodes_df[nodes_df.partition.isin(top5_part)].index . G_sub = nx.subgraph(G, top5_part_nodes) . nodes_df_top5 = nodes_df[nodes_df.partition.isin(top5_part)] . nodes_df_top5.head(2) . inventor dgr eig partition degree_basic . 0 ANKERSEN MICHAEL | 0.002273 | 0.023971 | 48 | 9 | . 1487 STIDSEN CARSTEN ENGGAARD | 0.001010 | 0.000872 | 48 | 4 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; # with the highest eigenvector centrality top_people = nodes_df_top5.groupby(&#39;partition&#39;)[&#39;eig&#39;].nlargest(5).reset_index() . top_people.rename({&#39;level_1&#39;:&#39;inventor_id&#39;}, axis=1, inplace=True) . top_people = pd.merge(top_people, df2[[&#39;inventor&#39;,&#39;inventor_id&#39;]].drop_duplicates(), on=&#39;inventor_id&#39;, how=&#39;inner&#39;) . top_people.head(50) . partition inventor_id eig inventor . 0 6 | 2359 | 0.138849 | GARIBAY PATRICK WILLIAM | . 1 6 | 2052 | 0.135706 | THOEGERSEN HENNING | . 2 6 | 1595 | 0.116120 | HOEG JENSEN THOMAS | . 3 6 | 1820 | 0.102756 | LAU JESPER F | . 4 6 | 1933 | 0.095319 | KRUSE THOMAS | . 5 12 | 1511 | 0.094768 | SAUERBERG PER | . 6 12 | 1531 | 0.089727 | PETTERSSON INGRID | . 7 12 | 1571 | 0.068005 | PETERSEN ANDERS KLARSKOV | . 8 12 | 1555 | 0.062463 | OLESEN PREBEN HOULBERG | . 9 12 | 449 | 0.052110 | KRUSE THOMAS | . 10 13 | 2089 | 0.002126 | BENGTSSON HENRIK | . 11 13 | 2030 | 0.002008 | EILERTSEN LARS | . 12 13 | 2087 | 0.001870 | DRUSTRUP JOERN | . 13 13 | 1028 | 0.000233 | JEPPESEN PER | . 14 13 | 2355 | 0.000225 | ANDRESEN LENE | . 15 21 | 1709 | 0.028325 | BREINHOLT JENS | . 16 21 | 515 | 0.025431 | JONASSEN IB | . 17 21 | 22 | 0.006666 | CHRISTENSEN THORKILD | . 18 21 | 2570 | 0.005346 | PATKAR SHAMKANT ANANT | . 19 21 | 1690 | 0.004938 | SVENDSEN ALLAN | . 20 48 | 1655 | 0.322390 | LAU JESPER | . 21 48 | 1762 | 0.207342 | KODRA JANOS TIBOR | . 22 48 | 1763 | 0.165824 | BEHRENS CARSTEN | . 23 48 | 2489 | 0.131985 | BLOCH PAW | . 24 48 | 1519 | 0.127359 | ANKERSEN MICHAEL | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Now we can explore the names, happy stalking. .",
            "url": "https://themuahdibportfolio.github.io/Blog/network%20analysis/intellectual%20property/patent%20data/2022/03/02/_02_08_PNA-(2).html",
            "relUrl": "/network%20analysis/intellectual%20property/patent%20data/2022/03/02/_02_08_PNA-(2).html",
            "date": " ‚Ä¢ Mar 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://themuahdibportfolio.github.io/Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Edoardo is a master‚Äôs student at Copenhagen Business School. His major is in Applied Economics and Finance, whereas his minor is in data science. .",
          "url": "https://themuahdibportfolio.github.io/Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://themuahdibportfolio.github.io/Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}